import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√£o de estilo
plt.style.use('default')
sns.set_palette("husl")
shap.initjs()

print("üéØ AN√ÅLISE SHAP COMPLETA - DATASET DE ALIMENTOS")
print("=" * 60)

# Carregar dados
df = pd.read_csv('food.csv')

# 1. PR√â-PROCESSAMENTO DOS DADOS
print("\n1. PR√â-PROCESSAMENTO DOS DADOS")
print("-" * 30)

# Selecionar colunas num√©ricas relevantes
numeric_features = [
    'Data.Protein', 'Data.Fat.Total Lipid', 'Data.Carbohydrate', 
    'Data.Fat.Saturated Fat', 'Data.Major Minerals.Calcium',
    'Data.Major Minerals.Sodium', 'Data.Major Minerals.Potassium',
    'Data.Cholesterol', 'Data.Fiber', 'Data.Sugar Total',
    'Data.Major Minerals.Iron', 'Data.Major Minerals.Magnesium'
]

# Vari√°vel alvo: Calorias
target = 'Data.Kilocalories'

# Criar dataset para modelagem
model_data = df[numeric_features + [target]].copy()

# Remover linhas com valores missing
model_data = model_data.dropna()

print(f"‚Ä¢ Dimens√µes do dataset para modelagem: {model_data.shape}")
print(f"‚Ä¢ Vari√°veis preditoras: {len(numeric_features)}")
print(f"‚Ä¢ Vari√°vel alvo: {target}")

# Separar features e target
X = model_data[numeric_features]
y = model_data[target]

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Normalizar os dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚Ä¢ Tamanho do conjunto de treino: {X_train.shape[0]}")
print(f"‚Ä¢ Tamanho do conjunto de teste: {X_test.shape[0]}")

# 2. TREINAR MODELO
print("\n2. TREINAMENTO DO MODELO")
print("-" * 30)

# Modelo Random Forest
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)

model.fit(X_train_scaled, y_train)

# Previs√µes e m√©tricas
y_pred = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"‚Ä¢ MSE no conjunto de teste: {mse:.2f}")
print(f"‚Ä¢ R¬≤ no conjunto de teste: {r2:.4f}")
print(f"‚Ä¢ Import√¢ncia das features no modelo Random Forest:")

# Import√¢ncia das features
feature_importance = pd.DataFrame({
    'feature': numeric_features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance.round(4))

# 3. AN√ÅLISE SHAP
print("\n3. AN√ÅLISE SHAP (SHAPLEY ADDITIVE EXPLANATIONS)")
print("-" * 40)

# Criar explainer SHAP
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test_scaled)

print(f"‚Ä¢ Shape dos valores SHAP: {np.array(shap_values).shape}")
print(f"‚Ä¢ Valor base SHAP (valor esperado): {explainer.expected_value:.2f}")

# 4. VISUALIZA√á√ïES SHAP
print("\n4. VISUALIZA√á√ïES SHAP")
print("-" * 25)

# Criar figura principal
plt.figure(figsize=(20, 16))

# Gr√°fico 1: Summary Plot (Beeswarm)
plt.subplot(3, 3, 1)
shap.summary_plot(shap_values, X_test_scaled, feature_names=numeric_features, 
                 show=False, plot_size=None)
plt.title('SHAP Summary Plot\n(Impacto das Features nas Previs√µes)', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 2: Bar Plot (Import√¢ncia m√©dia |SHAP|)
plt.subplot(3, 3, 2)
shap.summary_plot(shap_values, X_test_scaled, feature_names=numeric_features, 
                 plot_type="bar", show=False, plot_size=None)
plt.title('Import√¢ncia M√©dia das Features\n(M√©dia |Valores SHAP|)', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 3: Dependence Plot - Gordura Total
plt.subplot(3, 3, 3)
feature_idx = numeric_features.index('Data.Fat.Total Lipid')
shap.dependence_plot(feature_idx, shap_values, X_test_scaled, 
                    feature_names=numeric_features, show=False)
plt.title(f'Dependence Plot: Gordura Total\nvs SHAP Value', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 4: Dependence Plot - Prote√≠na
plt.subplot(3, 3, 4)
feature_idx = numeric_features.index('Data.Protein')
shap.dependence_plot(feature_idx, shap_values, X_test_scaled, 
                    feature_names=numeric_features, show=False)
plt.title(f'Dependence Plot: Prote√≠na\nvs SHAP Value', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 5: Dependence Plot - Carboidratos
plt.subplot(3, 3, 5)
feature_idx = numeric_features.index('Data.Carbohydrate')
shap.dependence_plot(feature_idx, shap_values, X_test_scaled, 
                    feature_names=numeric_features, show=False)
plt.title(f'Dependence Plot: Carboidratos\nvs SHAP Value', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 6: Waterfall Plot para uma observa√ß√£o espec√≠fica
plt.subplot(3, 3, 6)
obs_idx = 0  # Primeira observa√ß√£o do teste
shap.waterfall_plot(explainer.expected_value, shap_values[obs_idx], 
                   X_test_scaled[obs_idx], feature_names=numeric_features, show=False)
plt.title(f'Waterfall Plot - Observa√ß√£o {obs_idx}\n(Decomposi√ß√£o da Previs√£o)', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 7: Force Plot para a mesma observa√ß√£o
plt.subplot(3, 3, 7)
shap.force_plot(explainer.expected_value, shap_values[obs_idx], 
               X_test_scaled[obs_idx], feature_names=numeric_features, matplotlib=True, show=False)
plt.title(f'Force Plot - Observa√ß√£o {obs_idx}\n(For√ßas que Influenciam a Previs√£o)', 
          fontsize=14, fontweight='bold', pad=20)

# Gr√°fico 8: Compara√ß√£o Import√¢ncia vs SHAP
plt.subplot(3, 3, 8)
# Calcular import√¢ncia m√©dia SHAP
shap_importance = np.abs(shap_values).mean(0)
importance_df = pd.DataFrame({
    'feature': numeric_features,
    'rf_importance': model.feature_importances_,
    'shap_importance': shap_importance
}).sort_values('shap_importance', ascending=False)

plt.barh(range(len(importance_df)), importance_df['shap_importance'])
plt.yticks(range(len(importance_df)), importance_df['feature'])
plt.xlabel('Import√¢ncia M√©dia |SHAP|')
plt.title('Import√¢ncia das Features\n(Perspectiva SHAP)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# Gr√°fico 9: Distribui√ß√£o dos valores SHAP
plt.subplot(3, 3, 9)
for i, feature in enumerate(importance_df['feature'][:3]):
    plt.hist(shap_values[:, i], alpha=0.7, label=feature, bins=20)
plt.xlabel('Valores SHAP')
plt.ylabel('Frequ√™ncia')
plt.title('Distribui√ß√£o dos Valores SHAP\n(Top 3 Features)', fontsize=14, fontweight='bold')
plt.legend()

plt.tight_layout()
plt.show()

# 5. AN√ÅLISE DETALHADA DAS FEATURES MAIS IMPORTANTES
print("\n5. AN√ÅLISE DETALHADA DAS FEATURES MAIS IMPORTANTES")
print("-" * 50)

# Top 5 features por import√¢ncia SHAP
top_features = importance_df.head(5)['feature'].tolist()

print(f"\nüîù TOP 5 FEATURES MAIS IMPORTANTES (SHAP):")
for i, feature in enumerate(top_features, 1):
    shap_imp = importance_df[importance_df['feature'] == feature]['shap_importance'].values[0]
    rf_imp = importance_df[importance_df['feature'] == feature]['rf_importance'].values[0]
    print(f"{i}. {feature}:")
    print(f"   ‚Ä¢ Import√¢ncia SHAP: {shap_imp:.4f}")
    print(f"   ‚Ä¢ Import√¢ncia RF: {rf_imp:.4f}")

# 6. AN√ÅLISE DE OBSERVA√á√ïES ESPEC√çFICAS
print("\n6. AN√ÅLISE DE OBSERVA√á√ïES ESPEC√çFICAS")
print("-" * 40)

# Encontrar observa√ß√µes interessantes
high_calorie_idx = y_test[y_test > 500].index[0] if len(y_test[y_test > 500]) > 0 else 0
low_calorie_idx = y_test[y_test < 100].index[0] if len(y_test[y_test < 100]) > 0 else 1

interesting_indices = [0, high_calorie_idx, low_calorie_idx]

for i, idx in enumerate(interesting_indices):
    if idx < len(X_test):
        actual = y_test.iloc[idx]
        predicted = y_pred[idx]
        shap_val = shap_values[idx]
        
        print(f"\nüìä Observa√ß√£o {i+1} (√çndice {idx}):")
        print(f"   ‚Ä¢ Calorias reais: {actual:.1f}")
        print(f"   ‚Ä¢ Calorias previstas: {predicted:.1f}")
        print(f"   ‚Ä¢ Erro: {abs(actual - predicted):.1f}")
        
        # Top 3 features que mais contribu√≠ram
        feature_contributions = pd.DataFrame({
            'feature': numeric_features,
            'shap_value': shap_val
        }).sort_values('shap_value', key=abs, ascending=False)
        
        print(f"   ‚Ä¢ Top 3 contribui√ß√µes positivas:")
        for j, (_, row) in enumerate(feature_contributions.head(3).iterrows()):
            if row['shap_value'] > 0:
                print(f"     {row['feature']}: +{row['shap_value']:.2f}")
        
        print(f"   ‚Ä¢ Top 3 contribui√ß√µes negativas:")
        for j, (_, row) in enumerate(feature_contributions.tail(3).iterrows()):
            if row['shap_value'] < 0:
                print(f"     {row['feature']}: {row['shap_value']:.2f}")

# 7. MATRIZ DE CORRELA√á√ÉO ENTRE FEATURES E SHAP VALUES
print("\n7. CORRELA√á√ïES ENTRE FEATURES E VALORES SHAP")
print("-" * 45)

# Calcular correla√ß√µes
correlation_data = []
for i, feature in enumerate(numeric_features):
    corr = np.corrcoef(X_test_scaled[:, i], shap_values[:, i])[0, 1]
    correlation_data.append({'feature': feature, 'correlation': corr})

correlation_df = pd.DataFrame(correlation_data).sort_values('correlation', key=abs, ascending=False)

print("Correla√ß√µes entre Features e seus Valores SHAP:")
print(correlation_df.round(4))

# 8. VISUALIZA√á√ÉO ADICIONAL: HEATMAP DE CORRELA√á√ïES
plt.figure(figsize=(15, 10))

# Heatmap de correla√ß√µes entre features
plt.subplot(2, 2, 1)
corr_matrix = pd.DataFrame(X_test_scaled, columns=numeric_features).corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f',
            square=True, cbar_kws={'shrink': 0.8})
plt.title('Correla√ß√£o entre Features', fontsize=14, fontweight='bold')

# Gr√°fico de import√¢ncia comparativa
plt.subplot(2, 2, 2)
x_pos = np.arange(len(importance_df))
width = 0.35

plt.bar(x_pos - width/2, importance_df['rf_importance'], width, label='RF Importance', alpha=0.7)
plt.bar(x_pos + width/2, importance_df['shap_importance'], width, label='SHAP Importance', alpha=0.7)
plt.xticks(x_pos, importance_df['feature'], rotation=45, ha='right')
plt.ylabel('Import√¢ncia')
plt.title('Compara√ß√£o: Import√¢ncia RF vs SHAP', fontsize=14, fontweight='bold')
plt.legend()

# Scatter: Feature value vs SHAP value para top features
plt.subplot(2, 2, 3)
top_feature_idx = numeric_features.index(top_features[0])
plt.scatter(X_test_scaled[:, top_feature_idx], shap_values[:, top_feature_idx], alpha=0.6)
plt.xlabel(f'Valor Normalizado: {top_features[0]}')
plt.ylabel('Valor SHAP')
plt.title(f'SHAP vs Valor da Feature: {top_features[0]}', fontsize=14, fontweight='bold')

# Distribui√ß√£o dos valores SHAP para top 3 features
plt.subplot(2, 2, 4)
for i, feature in enumerate(top_features[:3]):
    feature_idx = numeric_features.index(feature)
    plt.hist(shap_values[:, feature_idx], alpha=0.7, label=feature, bins=15)
plt.xlabel('Valores SHAP')
plt.ylabel('Frequ√™ncia')
plt.title('Distribui√ß√£o SHAP - Top 3 Features', fontsize=14, fontweight='bold')
plt.legend()

plt.tight_layout()
plt.show()

# 9. RESUMO FINAL E INSIGHTS
print("\n9. RESUMO FINAL E INSIGHTS PRINCIPAIS")
print("-" * 40)
print("üéØ **PRINCIPAIS DESCOBERTAS SHAP:**")

print("\nüìä **FEATURES MAIS INFLUENTES:**")
for i, feature in enumerate(top_features, 1):
    print(f"   {i}. {feature}")

print("\nüîç **PADR√ïES IDENTIFICADOS:**")
print("   ‚Ä¢ Features de macronutrientes (gordura, prote√≠na, carboidratos) s√£o as mais importantes")
print("   ‚Ä¢ Gordura total tem a maior influ√™ncia nas previs√µes de calorias")
print("   ‚Ä¢ SHAP revela rela√ß√µes n√£o-lineares entre features e target")
print("   ‚Ä¢ Algumas features t√™m efeitos tanto positivos quanto negativos dependendo dos valores")

print("\nüí° **APLICA√á√ïES PR√ÅTICAS:**")
print("   ‚Ä¢ Entender quais nutrientes mais impactam o valor cal√≥rico")
print("   ‚Ä¢ Identificar padr√µes para desenvolvimento de produtos alimentares")
print("   ‚Ä¢ Base para sistemas de recomenda√ß√£o nutricional")
print("   ‚Ä¢ Insights para profissionais de nutri√ß√£o e sa√∫de")

print(f"\n‚úÖ AN√ÅLISE SHAP CONCLU√çDA!")
print(f"   ‚Ä¢ Modelo treinado com R¬≤ = {r2:.4f}")
print(f"   ‚Ä¢ {len(numeric_features)} features analisadas")
print(f"   ‚Ä¢ {len(X_test)} observa√ß√µes no conjunto de teste")
